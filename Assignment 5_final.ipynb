{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87fee377",
   "metadata": {},
   "source": [
    "# Assignment 5: PCA and Clustering (40 marks)\n",
    "### Due: December 6 at 11:59pm \n",
    "(Dropbox will remain open until December 8 at 11:59pm)\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7f05d0",
   "metadata": {},
   "source": [
    "The purpose of this assignment is to practice using PCA and clustering techniques on a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be4299ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c84e8e3",
   "metadata": {},
   "source": [
    "## Part 1: Principle Component Analysis (PCA) (18 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7cfa4f",
   "metadata": {},
   "source": [
    "### Step 1. Load data (2 marks)\n",
    "\n",
    "You have been asked by an agricultural company to help them predict the type of wheat based on kernel characteristics. You have been given all the information you need in the seeds_dataset.txt file on D2L. The original dataset can be found [here](https://archive.ics.uci.edu/dataset/236/seeds)\n",
    "\n",
    "The first step is to read in the file using pandas and inspect the first few columns. Note that the data is in a text file, so the separator is different compared to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "474481f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      1       2      3      4      5      6  7\n",
      "0  15.26  14.84  0.8710  5.763  3.312  2.221  5.220  1\n",
      "1  14.88  14.57  0.8811  5.554  3.333  1.018  4.956  1\n",
      "2  14.29  14.09  0.9050  5.291  3.337  2.699  4.825  1\n",
      "3  13.84  13.94  0.8955  5.324  3.379  2.259  4.805  1\n",
      "4  16.14  14.99  0.9034  5.658  3.562  1.355  5.175  1\n",
      "                0           1           2           3           4           5  \\\n",
      "count  210.000000  210.000000  210.000000  210.000000  210.000000  210.000000   \n",
      "mean    14.847524   14.559286    0.870999    5.628533    3.258605    3.700201   \n",
      "std      2.909699    1.305959    0.023629    0.443063    0.377714    1.503557   \n",
      "min     10.590000   12.410000    0.808100    4.899000    2.630000    0.765100   \n",
      "25%     12.270000   13.450000    0.856900    5.262250    2.944000    2.561500   \n",
      "50%     14.355000   14.320000    0.873450    5.523500    3.237000    3.599000   \n",
      "75%     17.305000   15.715000    0.887775    5.979750    3.561750    4.768750   \n",
      "max     21.180000   17.250000    0.918300    6.675000    4.033000    8.456000   \n",
      "\n",
      "                6           7  \n",
      "count  210.000000  210.000000  \n",
      "mean     5.408071    2.000000  \n",
      "std      0.491480    0.818448  \n",
      "min      4.519000    1.000000  \n",
      "25%      5.045000    1.000000  \n",
      "50%      5.223000    2.000000  \n",
      "75%      5.877000    3.000000  \n",
      "max      6.550000    3.000000  \n",
      "\n",
      "Missing value: \n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import dataset and inspect the first five columns\n",
    "df = pd.read_csv(\"seeds_dataset.txt\", sep='\\t', header = None)\n",
    "print(df.head(5))\n",
    "print(df.describe())\n",
    "print('\\nMissing value: ')\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0453ae3",
   "metadata": {},
   "source": [
    "Next, you will need to separate the data into your feature matrix and target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a5c76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      1       2      3      4      5      6\n",
      "0  15.26  14.84  0.8710  5.763  3.312  2.221  5.220\n",
      "1  14.88  14.57  0.8811  5.554  3.333  1.018  4.956\n",
      "2  14.29  14.09  0.9050  5.291  3.337  2.699  4.825\n",
      "3  13.84  13.94  0.8955  5.324  3.379  2.259  4.805\n",
      "4  16.14  14.99  0.9034  5.658  3.562  1.355  5.175\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: 7, dtype: int64\n",
      "Dimension of: \n",
      "\t Feature Matrix:  (210, 7)\n",
      "\t Target vector:  (210,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Separate the data into feature matrix and target vector. Print the shape of X and y\n",
    "X = df.drop([7], axis=1)\n",
    "y = df[7]\n",
    "print(X.head(5))\n",
    "print(y.head(5))\n",
    "print('Dimension of: ')\n",
    "print('\\t Feature Matrix: ', X.shape)\n",
    "print('\\t Target vector: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5024bec",
   "metadata": {},
   "source": [
    "### Step 2: Preprocessing (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908b657",
   "metadata": {},
   "source": [
    "To preprocess the data, we will need to scale the data. Which scaler would you select and why? *(1 mark)* \n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29892b5",
   "metadata": {},
   "source": [
    "Before we implement the scaler, we need to split the data into training and testing sets first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04f3a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f22529a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training feature matrix:  (157, 7)\n",
      "Shape of training target vector: (157,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print the shape of the training feature matrix and target vector\n",
    "print(\"Shape of training feature matrix: \", X_train.shape)\n",
    "print(\"Shape of training target vector:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282cbc43",
   "metadata": {},
   "source": [
    "Now that we have our training data, we can implement our selected scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed75b036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "#scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa23f3",
   "metadata": {},
   "source": [
    "### Step 3: Model Selection (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773d0687",
   "metadata": {},
   "source": [
    "The next step is to instantiate our selected model. In this case, we will be using `LogisticRegression()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b79801bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "#lr = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2177add4",
   "metadata": {},
   "source": [
    "### Step 4: Validation (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092329e",
   "metadata": {},
   "source": [
    "To validate the results, we need to use cross-validation. To make sure we are using the best hyperparmeters, we can use a grid search. The parameter grid has been provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3e5cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1.0, 10.0],\n",
    "             'fit_intercept': [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0317e3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'C' for estimator Pipeline(steps=[('scaler', StandardScaler()),\n                ('classifier', LogisticRegression())]). Valid parameters are: ['memory', 'steps', 'verbose'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, scaler), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m, model)])\n\u001b[0;32m     12\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid, cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, scoring \u001b[38;5;241m=\u001b[39m scoring, refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     16\u001b[0m best_param \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest paramters: \u001b[39m\u001b[38;5;124m'\u001b[39m, best_param)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:720\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m parameters\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    718\u001b[0m         cloned_parameters[k] \u001b[38;5;241m=\u001b[39m clone(v, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 720\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcloned_parameters)\n\u001b[0;32m    722\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    724\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\pipeline.py:215\u001b[0m, in \u001b[0;36mPipeline.set_params\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    197\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set the parameters of this estimator.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m    Valid parameter keys can be listed with ``get_params()``. Note that\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m        Pipeline class instance.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_params(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py:68\u001b[0m, in \u001b[0;36m_BaseComposition._set_params\u001b[1;34m(self, attr, **params)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replace_estimator(attr, name, params\u001b[38;5;241m.\u001b[39mpop(name))\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# 3. Step parameters and other initialisation arguments\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ensf-ml\\Lib\\site-packages\\sklearn\\base.py:229\u001b[0m, in \u001b[0;36mBaseEstimator.set_params\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m valid_params:\n\u001b[0;32m    228\u001b[0m     local_valid_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_param_names()\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for estimator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValid parameters are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_valid_params\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m     )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m delim:\n\u001b[0;32m    235\u001b[0m     nested_params[key][sub_key] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter 'C' for estimator Pipeline(steps=[('scaler', StandardScaler()),\n                ('classifier', LogisticRegression())]). Valid parameters are: ['memory', 'steps', 'verbose']."
     ]
    }
   ],
   "source": [
    "#TODO: Implement the grid search and print the best parameters, cross-validation score and test score\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_score': make_scorer(f1_score)\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([('scaler', scaler), ('classifier', model)])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv = 5, scoring = scoring, refit='f1_score')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_param = grid_search.best_params_\n",
    "print('Best paramters: ', best_param)\n",
    "result = grid_search.results_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cee662",
   "metadata": {},
   "source": [
    "### Step 5: Visualize using PCA (3 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed3fd7",
   "metadata": {},
   "source": [
    "Now you can visualize the results from Step 4 using PCA. Use the best parameters from the previous step to predict the label for the testing data. For the plot, each data point must be colored based on the class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc2310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predict y labels using best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3416d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Reduce dimensions of test data for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5317451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot test points colored by predicted label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ea401",
   "metadata": {},
   "source": [
    "#### Questions (4 marks)\n",
    "\n",
    "1. Did the logistic regression model work well for this dataset? Why would you make this conclusion? *(2 marks)*\n",
    "1. Did PCA work well for this model? How does this relate to the results from the model used? *(2 marks)*\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88a3cf1",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8336341",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf578b1",
   "metadata": {},
   "source": [
    "## Part 2. Implement clustering (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937cb37b",
   "metadata": {},
   "source": [
    "The agricultural company approaches you again because they lost all the labels for their data. Use the Kmeans clustering method to assign labels to the seeds dataset used in Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980c62e1",
   "metadata": {},
   "source": [
    "### Step 1: Cluster using original data (6 marks)\n",
    "\n",
    "This part of the assignment asks you to implement Kmeans clustering using the orginal feature matrix (after scaling has been performed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c441df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scale feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efb7a1b",
   "metadata": {},
   "source": [
    "The first step is to use the elbow method to determine the optimal number of clusters. In this case, you can use the distortion metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f85da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the elbow method to determine the optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be361544",
   "metadata": {},
   "source": [
    "Next, implement Kmeans and calculate the following validation metrics:\n",
    "- Silhouette score\n",
    "- Calinski-Harabasz score\n",
    "- Davies-Bouldin score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da40bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement K-means with the number of clusters determined in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085dc51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the three different validation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e40c04",
   "metadata": {},
   "source": [
    "Plot the clustering results using PCA. Remember to color each data point based on the cluster label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47012908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot clustering results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48df428e",
   "metadata": {},
   "source": [
    "### Step 2: Cluster using PCA-transformed data (4 marks)\n",
    "\n",
    "Implement Kmeans clustering using PCA-transformed data. The first step is to use the explained variance ratio to find the number of components required to explain at least 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0a5d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the number of principal components using explained variance ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af06a41",
   "metadata": {},
   "source": [
    "Next, transform the data using PCA, implement Kmeans and calculate the three validation metrics used in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed38b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use PCA to transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement K-means with the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a3bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the three different validation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb5219",
   "metadata": {},
   "source": [
    "### Step 3: Display results (2 marks)\n",
    "\n",
    "Print the results for Steps 2 and 3 in a table. Include column and row labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c945947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1086de9",
   "metadata": {},
   "source": [
    "#### Questions (4 marks)\n",
    "\n",
    "1. Did the optimal number of clusters make sense based on the context? *(1 mark)*\n",
    "1. Compare the PCA plot from Part 2 step 1 to the PCA plot from Part 1. Did we get similar results? *(1 mark)*\n",
    "1. How many components did you use for the PCA analysis? Did using PCA improve your results? Why or why not? *(2 marks)*\n",
    "\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521489ee",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ae4b12",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e42ec7",
   "metadata": {},
   "source": [
    "## Part 3: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9755613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
